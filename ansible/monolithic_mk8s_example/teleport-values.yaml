appTeleport:
  enabled: true
  # deploy all apps to this cluster
  address: https://kubernetes.default.svc
  namespace: argocd
  argocd:
    namespace: argocd
    project: default

  source:
    repoURL: https://harbor.ukserp.ac.uk/chartrepo/dare
    chart: teleport
    targetRevision: 1.5.1

  values:
    appGuacamole:
      # don't want Guac for this deployment
      enabled: false

    appJupyter:
      enabled: true
      namespace: jupyter
      source:
        repoURL: https://hub.jupyter.org/helm-chart/
        chart: jupyterhub
        targetRevision: 3.1.0
      values:
        singleuser:
          lifecycleHooks:
            postStart:
              exec:
                command:
                  - "sh"
                  - "-c"
                  - >
                    /usr/local/bin/start-notebook.d/session_startup.sh
          networkPolicy:
            enabled: false
          defaultUrl: "/lab"
          extraEnv:
            JUPYTERHUB_SINGLEUSER_APP: "jupyter_server.serverapp.ServerApp"
            # put your pip/cran proxy/mirror address in the below variables
            NEXUS_HTTP_SCHEME: "http"
            NEXUS_PROXY_ADDR: "192.168.7.56:8081"
          profileList:
          - display_name: "basic image"
            kubespawner_override:
              image: "harbor.ukserp.ac.uk/dare/jupyter:pr-59-2f7bf0e"
              default_url: '/lab'
              cmd: ['jupyterhub-singleuser', '--ContentsManager.allow_hidden=True']

    appTrino:
      enabled: true
      namespace: trino
      source:
        repoURL: https://harbor.ukserp.ac.uk/chartrepo/dare
        chart: trino
        targetRevision: 0.0.0-pr.63
      values:
        image:
          repository: harbor.ukserp.ac.uk/dare/trino
          tag: 1.5.0

        server:
          workers: 2
          config:
            spillToDisk:
              enabled: false
            # THIS IS THE NEW SECTION
            accessControl:
              enabled: true
              properties:
                access-control.name: "tech.stackable.trino.opa.OpaAuthorizer"
                # This assumes that opa-operator is deployed in the same cluster as Trino, in a namespace called "opa-operator".
                # If your opa operator is deployed in a different namespace, then you'll need to change this.
                opa.policy.uri: "http://opa-cluster.dare-opa.svc.cluster.local:8081/v1/data/trino/allow"
                security.refresh-period: "15s"
            query:
              maxMemory: "2GB"
              # maxmemorypernode + heapHeadroomPerNode <= coordination|worker.jvm.maxheapsize
              maxMemoryPerNode: "2GB"
            memory:
              heapHeadroomPerNode: "1GB"
            https:
              enabled: false
          autoscaling:
            enabled: false
          coordinatorExtraConfig: |
            http-server.process-forwarded=true
            internal-communication.shared-secret=thisisdefinitelyasecret
          workerExtraConfig: |
            http-server.process-forwarded=true
            internal-communication.shared-secret=thisisdefinitelyasecret

        coordinator:
          jvm:
            maxHeapSize: "3G"

        worker:
          jvm:
            maxHeapSize: "3G"

        service:
          type: LoadBalancer

        additionalCatalogs:
          # In the below blocks, make sure you change these:
          # hive.s3.aws-access-key=06LVN2MwcfLzsFr7BZ
          # hive.s3.aws-secret-key=SrOMOsnLa8gTHWvEUk3hNxZP
          # to the same values as in the appMinioTenant.values.secrets.accessKey and secretKey
          minio: |
            connector.name = hive
            hive.metastore.uri = thrift://app-hive:9083
            hive.metastore.thrift.delete-files-on-drop=true
            hive.s3.path-style-access=true
            hive.s3.endpoint=http://minio.dare-minio-tenant.svc.cluster.local:80
            hive.s3.aws-access-key=06LVN2MwcfLzsFr7BZ
            hive.s3.aws-secret-key=SrOMOsnLa8gTHWvEUk3hNxZP
            hive.non-managed-table-writes-enabled=true
            hive.s3select-pushdown.enabled=true
            hive.storage-format=ORC
            hive.allow-drop-table=true
          iceberg: |
            connector.name = hive
            hive.metastore.uri = thrift://app-hive:9083
            hive.metastore.thrift.delete-files-on-drop=true
            hive.s3.path-style-access=true
            hive.s3.endpoint=http://minio.dare-minio-tenant.svc.cluster.local:80
            hive.s3.aws-access-key=06LVN2MwcfLzsFr7BZ
            hive.s3.aws-secret-key=SrOMOsnLa8gTHWvEUk3hNxZP
            hive.non-managed-table-writes-enabled=true
            hive.s3select-pushdown.enabled=true
            hive.storage-format=ORC
            hive.allow-drop-table=true
          # !!! This is where you'd define the remote trino connection !!! 
          # !!! connection-password is optional !!!
          # epcc: |
          #   connector.name = trino
          #   connection-url=jdbc:trino://trino.dare.epcc.dk.serp.ac.uk:443/tpcds/sf3000
          #   connection-user=${ENV:trinoremote_username}
          #   connection-password=${ENV:trinoremote_password}
          #   join-pushdown.enabled=true
          #   join-pushdown.strategy=EAGER
          #   unsupported-type-handling=CONVERT_TO_VARCHAR


    appHive:
      enabled: true
      # hive needs to go into the trino namespace
      namespace: trino
      source:
        repoURL: https://harbor.ukserp.ac.uk/chartrepo/dare
        chart: hive
        targetRevision: 1.3.3
      values:
        image:
          name: harbor.ukserp.ac.uk/dare/hive
          tag: "1.3.3"

        # In the below block, make sure you change these:
        # accessKey: 06LVN2MwcfLzsFr7BZ
        # secretKey: SrOMOsnLa8gTHWvEUk3hNxZP
        # to the same values as in the appMinioTenant.values.secrets.accessKey and secretKey
        minio:
          accessKey: 06LVN2MwcfLzsFr7BZ
          secretKey: SrOMOsnLa8gTHWvEUk3hNxZP
          endpoint: http://minio.dare-minio-tenant.svc.cluster.local:80

        mysql:
          architecture: replication
          auth:
            rootPassword: 'P4ssW0rd!1'
            createDatabase: true
            database: metastore_db
            username: hive
            password: 'H1v3P4ssW0rd!1'
            replicationPassword: 'R3pl1c4t1onP4ssW0rd!1'

          secondary:
            replicaCount: 1

          metrics:
            enabled: false

    appMinioTenant:
      enabled: true
      namespace: minio-tenant
      source:
        repoURL: https://operator.min.io/
        chart: tenant
        targetRevision: 5.0.6
      values:
        secrets:
          name: tmp-minio-env-config
          accessKey: "06LVN2MwcfLzsFr7BZ"
          secretKey: "SrOMOsnLa8gTHWvEUk3hNxZP"

        tenant:
          name: dare-teleport
          configuration:
            name: tmp-minio-env-config

          pools:
            # define your storage pool for minio
            # this can be tricky, it may help to look at the MinIO pool size calculator
            - servers: 1
              name: pool-0
              volumesPerServer: 1
              size: 50Gi 

          metrics:
            enabled: false

          certificate:
            requestAutoCert: false

          prometheusOperator: false

    # dont deploy Vault with this chart, we deployed it with ansible
    appVault:
      enabled: false

    # DO deploy OPA
    appOpa:
      enabled: true
      namespace: opa
      source:
        repoURL: harbor.ukserp.ac.uk/dare/chart
        chart: opa
        targetRevision: 1.4.6
      values:
        rules:
          host.rego: |
            package trino

            import future.keywords.if
            import future.keywords.in

            # as this is a sample allow rule, it just lets anyone in
            default allow := true


    # this app isn't complete yet
    appControl:
      enabled: false

    # don't deploy Keycloak as nothing set up for oidc auth
    appKeycloak:
      enabled: false
      namespace: keycloak
      source:
        repoURL: https://charts.bitnami.com/bitnami
        chart: keycloak
        targetRevision: 15.1.6

    appPromStack:
      enabled: true
      namespace: monitoring
      source:
        repoURL: https://prometheus-community.github.io/helm-charts
        chart: kube-prometheus-stack
        targetRevision: 46.3.0

    appLoki:
      enabled: true
      namespace: monitoring
      source:
        repoURL: https://grafana.github.io/helm-charts
        chart: loki-stack
        targetRevision: 2.9.10
      values:
        loki:
          commonConfig:
            replication_factor: 1
          auth_enabled: false
          storage:
            type: 'filesystem'
          test:
            enabled: false
          # need to specify this or it breaks Grafana
          isDefault: false
